# ğŸ¤– Personal RAG System: Local AI Assistant with ChromaDB, LLM & Telegram Bot

This is a fully local Retrieval-Augmented Generation (RAG) system that combines:

* ğŸ§  **LLM for generation** (e.g. Phi-3, LLaMA, Mistral, etc., running locally via `llama.cpp`)
* ğŸ“š **Embedding model** for semantic document retrieval
* ğŸ’¾ **ChromaDB** for vector storage
* ğŸŒ **Modern Web Frontend** for querying
* ğŸ’¬ **Telegram Bot** interface

> ğŸ” Entirely private and offline â€” no OpenAI/Anthropic APIs. Your data stays with you.

---

## ğŸ“½ï¸ Demo Video (Frontend Interaction)

ğŸš§ *Placeholder: Demo video will be embedded here.*

```markdown
[![Watch the demo](https://img.youtube.com/vi/VIDEO_ID/0.jpg)](https://www.youtube.com/watch?v=VIDEO_ID)
```

---

## ğŸ’¬ Demo Video (Telegram Bot)

ğŸš§ *Placeholder: Screen recording of the Telegram bot in action.*

---

## ğŸ§© Features

* ğŸ” Smart document retrieval via embeddings
* âœ¨ Local LLM-based answer generation
* ğŸ“¥ Easy file ingestion (PDF, text, etc.)
* ğŸŒ Web-based UI for real-time Q\&A
* ğŸ¤– Telegram bot interface
* âš¡ Fast, streaming responses via FastAPI

---

## ğŸ—ï¸ Architecture

```text
         +-----------------+
         | Telegram Bot UI |
         +--------+--------+
                  |
+---------+       v       +--------------+
| Web UI  | ---> FastAPI <-> Retrieval   |
+---------+       |       +--------------+
                  v
        +--------------------+
        | ChromaDB (VectorDB)|
        +--------------------+
                  |
                  v
        +--------------------+
        | Embedding Model    |
        +--------------------+
                  |
                  v
        +--------------------+
        | Local LLM (GGUF)   |
        +--------------------+
```

---

## ğŸš€ Setup Instructions (Local)

> âœ… Tested on **Linux/macOS**. For Windows, **WSL is required** due to `llama.cpp` dependencies like CMake & g++.

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/local-rag-system.git
cd local-rag-system
```

### 2. Create a Virtual Environment

```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Install Dependencies (\~2GB)

```bash
pip install -r requirements.txt
```

> âš ï¸ This installs models like `sentence-transformers` and dependencies for llama.cpp (if needed).

### 4. Ingest Your Documents

```bash
python app/ingest.py  # or your custom ingestion script
```

### 5. Run the Backend API

```bash
python run.py  # Or use uvicorn: uvicorn app.main:app --reload
```

### 6. Start the Telegram Bot (optional)

```bash
python telegram_bot.py
```

---

## ğŸ³ Docker Support (For Users)

> ğŸ’¡ If you're running this on a different OS or want zero setup: Docker is the easiest way.

### ğŸ› ï¸ Prerequisites

Make sure you have [Docker installed](https://www.docker.com/products/docker-desktop/).

### ğŸ“¦ Build Docker Image

```bash
docker build -t local-rag .
```

### â–¶ï¸ Run Docker Container

```bash
docker run -p 8000:8000 local-rag
```

> ğŸ“ The container runs the FastAPI backend. You can expose additional ports (e.g., for frontend or Telegram bot) as needed.

---

## ğŸ“ Folder Structure

```bash
local-rag-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ retrieval.py            # Embedding search
â”‚   â”œâ”€â”€ generation.py           # LLM generation logic
â”‚   â”œâ”€â”€ ingest.py               # Ingest docs into ChromaDB
â”‚   â””â”€â”€ main.py                 # FastAPI routes
â”œâ”€â”€ frontend/                   # Web frontend code (React/Vite)
â”œâ”€â”€ telegram_bot.py             # Telegram bot entrypoint
â”œâ”€â”€ run.py                      # Main runner
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .dockerignore
â””â”€â”€ README.md
```

---

## ğŸ” FAQs

### Why not run it using Docker myself?

This repo is **developed using a local Python environment (in WSL)** for simplicity. Docker is provided for:

* Other users/recruiters
* Cross-platform compatibility

No need for the developer to use Docker unless desired.

### What about model downloads?

Model download automation is out-of-scope (due to licenses & size). You can:

* Place `.gguf` model in a `models/` folder
* Adjust the code to point to your local path

---

## ğŸ“« Contact

Built with â¤ï¸ by \Sharif.

If you're a recruiter or engineer interested in building real-world AI tools, feel free to reach out:

* ğŸ“§ Email: [you@example.com](mailto:you@example.com)
* ğŸ’¼ LinkedIn: [linkedin.com/in/yourname](https://linkedin.com/in/yourname)

---

## â­ï¸ Bonus (If You Want to Extend)

* ğŸ“Š Add tracing with Langfuse or Prometheus + Grafana
* ğŸ” Secure the API using OAuth2 or JWT
* ğŸŒ Deploy with Docker Compose, Traefik, or on Hugging Face Spaces
* ğŸ“¥ Upload files via frontend drag-n-drop
* ğŸ§  Switch to better embeddings (e.g. `bge-m3` or `E5`)
